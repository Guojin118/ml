{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：安装fasttext\n",
    "可以参考：https://github.com/facebookresearch/fastText#building-fasttext\n",
    "* https://blog.csdn.net/weixin_43977375/article/details/90200837\n",
    "* conda install -c conda-forge xgboost\n",
    "* conda install -c conda-forge fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步： 准备数据集\n",
    "\n",
    "* 我这里用的是清华的新闻数据集（由于完整数据集较大，这里只取部分数据）\n",
    "* 数据链接：点击获取[网盘数据](https://pan.baidu.com/s/1Pj8F3xVVb3hnBt8p2qzKlQ) 提取码：byoi(data.txt为数据集，stopwords.txt为停用词)\n",
    "* 下载好后的数据格式为：\n",
    "* 对应的标签分别为(由于只是用小部分数据，所以data.txt只包含部分标签)：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_tag = {\n",
    "    '财经': 'Finance',\n",
    "    '彩票': 'Lottery',\n",
    "    '房产': 'Property',\n",
    "    '股票': 'Shares',\n",
    "    '家居': 'Furnishing',\n",
    "    '教育': 'Education',\n",
    "    '科技': 'Technology',\n",
    "    '社会': 'Sociology',\n",
    "    '时尚': 'Fashion',\n",
    "    '时政': 'Affairs',\n",
    "    '体育': 'Sports',\n",
    "    '星座': 'Constellation',\n",
    "    '游戏': 'Game',\n",
    "    '娱乐': 'Entertainment'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：数据预处理\n",
    "* 由于data.txt已经经过了分词和去停用词的处理，所以这里只需要对数据进行切割为训练集和测试集即可。\n",
    "* 分词和去停用词的工具代码(运行时不需要执行此部分代码)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 2.022 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from types import MethodType, FunctionType\n",
    "\n",
    "import jieba\n",
    "\n",
    "\n",
    "def clean_txt(raw):\n",
    "    fil = re.compile(r\"[^0-9a-zA-Z\\u4e00-\\u9fa5]+\")\n",
    "    return fil.sub(' ', raw)\n",
    "\n",
    "def seg(sentence, sw, apply=None):\n",
    "    if isinstance(apply, FunctionType) or isinstance(apply, MethodType):\n",
    "        sentence = apply(sentence)\n",
    "    return ' '.join([i for i in jieba.cut(sentence) if i.strip() and i not in sw])\n",
    "    \n",
    "def stop_words():\n",
    "    with open('stopwords.txt', 'r', encoding='utf-8') as swf:\n",
    "        return [line.strip() for line in swf]\n",
    "\n",
    "# 对某个sentence进行处理：\n",
    "content = '上海天然橡胶期价周三再创年内新高，主力合约突破21000元/吨重要关口。'\n",
    "res = seg(content.lower().replace('\\n', ''), stop_words(), apply=clean_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#切割数据(这里我是先将txt文件转换成csv文件，方便后面的计算)\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class _MD(object):\n",
    "    mapper = {\n",
    "        str: '',\n",
    "        int: 0,\n",
    "        list: list,\n",
    "        dict: dict,\n",
    "        set: set,\n",
    "        bool: False,\n",
    "        float: .0\n",
    "    }\n",
    "\n",
    "    def __init__(self, obj, default=None):\n",
    "        self.dict = {}\n",
    "        assert obj in self.mapper, \\\n",
    "            'got a error type'\n",
    "        self.t = obj\n",
    "        if default is None:\n",
    "            return\n",
    "        assert isinstance(default, obj), \\\n",
    "            f'default ({default}) must be {obj}'\n",
    "        self.v = default\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.dict[key] = value\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if item not in self.dict and hasattr(self, 'v'):\n",
    "            self.dict[item] = self.v\n",
    "            return self.v\n",
    "        elif item not in self.dict:\n",
    "            if callable(self.mapper[self.t]):\n",
    "                self.dict[item] = self.mapper[self.t]()\n",
    "            else:\n",
    "                self.dict[item] = self.mapper[self.t]\n",
    "            return self.dict[item]\n",
    "        return self.dict[item]\n",
    "\n",
    "\n",
    "def defaultdict(obj, default=None):\n",
    "    return _MD(obj, default)\n",
    "\n",
    "\n",
    "class TransformData(object):\n",
    "    def to_csv(self, handler, output, index=False):\n",
    "        dd = defaultdict(list)\n",
    "        for line in handler:\n",
    "            label, content = line.split(',', 1)\n",
    "            dd[label.strip('__label__').strip()].append(content.strip())\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for key in dd.dict:\n",
    "            col = pd.Series(dd[key], name=key)\n",
    "            df = pd.concat([df, col], axis=1)\n",
    "        return df.to_csv(output, index=index, encoding='utf-8')\n",
    "\n",
    "\n",
    "def split_train_test(source, auth_data=False):\n",
    "    if not auth_data:\n",
    "        train_proportion = 0.8\n",
    "    else:\n",
    "        train_proportion = 0.98\n",
    "\n",
    "    basename = source.rsplit('.', 1)[0]\n",
    "    train_file = basename + '_train.txt'\n",
    "    test_file = basename + '_test.txt'\n",
    "\n",
    "    handel = pd.read_csv(source, index_col=False, low_memory=False)\n",
    "    train_data_set = []\n",
    "    test_data_set = []\n",
    "    for head in list(handel.head()):\n",
    "        train_num = int(handel[head].dropna().__len__() * train_proportion)\n",
    "        sub_list = [f'__label__{head} , {item.strip()}\\n' for item in handel[head].dropna().tolist()]\n",
    "        train_data_set.extend(sub_list[:train_num])\n",
    "        test_data_set.extend(sub_list[train_num:])\n",
    "    shuffle(train_data_set)\n",
    "    shuffle(test_data_set)\n",
    "\n",
    "    with open(train_file, 'w', encoding='utf-8') as trainf,\\\n",
    "        open(test_file, 'w', encoding='utf-8') as testf:\n",
    "        for tds in train_data_set:\n",
    "            trainf.write(tds)\n",
    "        for i in test_data_set:\n",
    "            testf.write(i)\n",
    "\n",
    "    return train_file, test_file\n",
    "\n",
    "# 转化成csv\n",
    "td = TransformData()\n",
    "handler = open('data.txt')\n",
    "td.to_csv(handler, 'data.csv')\n",
    "handler.close()\n",
    "\n",
    "# 将csv文件切割，会生成两个文件（data_train.txt和data_test.txt）\n",
    "train_file, test_file = split_train_test('data.csv', auth_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步：训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989, 0.9797775530839231, 0.9797775530839231)\n"
     ]
    }
   ],
   "source": [
    "import fasttext.FastText as fasttext\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def train_model(ipt=None, opt=None, model='', dim=100, epoch=5, lr=0.1, loss='softmax'):\n",
    "    np.set_printoptions(suppress=True)\n",
    "    if os.path.isfile(model):\n",
    "        classifier = fasttext.load_model(model)\n",
    "    else:\n",
    "        classifier = fasttext.train_supervised(ipt, label='__label__', dim=dim, epoch=epoch,\n",
    "                                         lr=lr, wordNgrams=2, loss=loss)\n",
    "        \"\"\"\n",
    "          训练一个监督模型, 返回一个模型对象\n",
    "          \n",
    "          @param input:           训练数据文件路径\n",
    "          @param lr:              学习率\n",
    "          @param dim:             向量维度\n",
    "          @param ws:              cbow模型时使用\n",
    "          @param epoch:           次数\n",
    "          @param minCount:        词频阈值, 小于该值在初始化时会过滤掉\n",
    "          @param minCountLabel:   类别阈值，类别小于该值初始化时会过滤掉\n",
    "          @param minn:            构造subword时最小char个数\n",
    "          @param maxn:            构造subword时最大char个数\n",
    "          @param neg:             负采样\n",
    "          @param wordNgrams:      n-gram个数\n",
    "          @param loss:            损失函数类型, softmax, ns: 负采样, hs: 分层softmax\n",
    "          @param bucket:          词扩充大小, [A, B]: A语料中包含的词向量, B不在语料中的词向量\n",
    "          @param thread:          线程个数, 每个线程处理输入数据的一段, 0号线程负责loss输出\n",
    "          @param lrUpdateRate:    学习率更新\n",
    "          @param t:               负采样阈值\n",
    "          @param label:           类别前缀\n",
    "          @param verbose:         ??\n",
    "          @param pretrainedVectors: 预训练的词向量文件路径, 如果word出现在文件夹中初始化不再随机\n",
    "          @return model object\n",
    "        \"\"\"\n",
    "        classifier.save_model(opt)\n",
    "    return classifier\n",
    "\n",
    "dim = 100\n",
    "lr = 5\n",
    "epoch = 5\n",
    "model = f'data_dim{str(dim)}_lr0{str(lr)}_iter{str(epoch)}.model'\n",
    "\n",
    "classifier = train_model(ipt='data_train.txt',\n",
    "                         opt=model,\n",
    "                         model=model,\n",
    "                         dim=dim, epoch=epoch, lr=0.5\n",
    "                         )\n",
    "\n",
    "result = classifier.test('data_test.txt')\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_precision_and_recall(file='data_test.txt'):\n",
    "    precision = defaultdict(int, 1)\n",
    "    recall = defaultdict(int, 1)\n",
    "    total = defaultdict(int, 1)\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            label, content = line.split(',', 1)\n",
    "            total[label.strip().strip('__label__')] += 1\n",
    "            labels2 = classifier.predict([seg(sentence=content.strip(), sw='', apply=clean_txt)])\n",
    "            pre_label, sim = labels2[0][0][0], labels2[1][0][0]\n",
    "            recall[pre_label.strip().strip('__label__')] += 1\n",
    "    \n",
    "            if label.strip() == pre_label.strip():\n",
    "                precision[label.strip().strip('__label__')] += 1\n",
    "    \n",
    "    print('precision', precision.dict)\n",
    "    print('recall', recall.dict)\n",
    "    print('total', total.dict)\n",
    "    for sub in precision.dict:\n",
    "        pre = precision[sub] / total[sub]\n",
    "        rec =  precision[sub] / recall[sub]\n",
    "        F1 = (2 * pre * rec) / (pre + rec)\n",
    "        print(f\"{sub.strip('__label__')}  precision: {str(pre)}  recall: {str(rec)}  F1: {str(F1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9885, 0.9741021750126454, 0.9741021750126454)\n",
      "precision {'Technology': 998, 'Education': 996, 'Affairs': 1000, 'Sports': 998, 'Sociology': 986, 'Gam': 869, 'Financ': 997, 'Furnishing': 998, 'Entertainment': 1001, 'Shares': 999}\n",
      "recall {'Technology': 999, 'Education': 1013, 'Affairs': 1002, 'Sports': 999, 'Sociology': 998, 'Gam': 872, 'Financ': 1000, 'Furnishing': 1003, 'Entertainment': 1006, 'Shares': 1003}\n",
      "total {'Technology': 1001, 'Education': 1001, 'Affairs': 1001, 'Sports': 1001, 'Sociology': 1001, 'Gam': 876, 'Financ': 1001, 'Furnishing': 1001, 'Entertainment': 1001, 'Shares': 1001, 'Property': 11}\n",
      "Technology  precision: 0.997002997002997  recall: 0.998998998998999  F1: 0.998\n",
      "Education  precision: 0.995004995004995  recall: 0.983218163869694  F1: 0.9890764647467727\n",
      "Affairs  precision: 0.999000999000999  recall: 0.998003992015968  F1: 0.9985022466300548\n",
      "Sports  precision: 0.997002997002997  recall: 0.998998998998999  F1: 0.998\n",
      "Sociology  precision: 0.985014985014985  recall: 0.9879759519038076  F1: 0.9864932466233116\n",
      "Gam  precision: 0.9920091324200914  recall: 0.9965596330275229  F1: 0.9942791762013731\n",
      "Financ  precision: 0.996003996003996  recall: 0.997  F1: 0.9965017491254373\n",
      "Furnishing  precision: 0.997002997002997  recall: 0.9950149551345963  F1: 0.9960079840319361\n",
      "Entertainment  precision: 1.0  recall: 0.9950298210735586  F1: 0.9975087194818136\n",
      "Shares  precision: 0.998001998001998  recall: 0.996011964107677  F1: 0.9970059880239521\n"
     ]
    }
   ],
   "source": [
    "def main(source):\n",
    "    basename = source.rsplit('.', 1)[0]\n",
    "    csv_file = basename + '.csv'\n",
    "\n",
    "    td = TransformData()\n",
    "    handler = open(source)\n",
    "    td.to_csv(handler, csv_file)\n",
    "    handler.close()\n",
    "\n",
    "    train_file, test_file = split_train_test(csv_file)\n",
    "\n",
    "    dim = 100\n",
    "    lr = 5\n",
    "    epoch = 5\n",
    "    model = f'data/data_dim{str(dim)}_lr0{str(lr)}_iter{str(epoch)}.model'\n",
    "\n",
    "    classifier = train_model(ipt=train_file,\n",
    "                             opt=model,\n",
    "                             model=model,\n",
    "                             dim=dim, epoch=epoch, lr=0.5\n",
    "                             )\n",
    "\n",
    "    result = classifier.test(test_file)\n",
    "    print(result)\n",
    "\n",
    "    cal_precision_and_recall(test_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main('data.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
